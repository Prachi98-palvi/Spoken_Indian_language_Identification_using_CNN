# -*- coding: utf-8 -*-
"""CNN_Slid.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CocOAFMOitkRoTSqV1cDYrV947PjvswG

# Mouting Drive to Store and Load data
"""

from google.colab import drive
drive.mount('/content/drive/')

"""# Importing Libraries"""

import tensorflow as tf
print(tf.__version__)

from tensorflow import keras
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Flatten, Dense, Dropout, BatchNormalization, Conv2D, MaxPooling2D, Input
from tensorflow.keras.models import Model
from keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization,Input
from keras import regularizers, optimizers
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.preprocessing import image

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
#import cv2

from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix
from tqdm import tqdm
from sklearn import metrics

import os

"""# Converting wav files to Spectrograms and saving them into different folders"""

DATADIR = '/content/drive/My Drive/project/newdata'
CATEGORIES = ["bengali","hindi","kannada","malayalam","marathi","tamil","telugu"]
X = []
y = []

imagecats = []

count = 0
catcount = 0

for category in CATEGORIES:
  count = 0  
  for img_path in os.listdir(DATADIR + "/"+category):    
    count = count+1   
    npath = DATADIR + "/" + category + "/" + img_path    
    imagecats.append(catcount)
    img = image.load_img(npath,grayscale='true',color_mode='rgb',target_size=(223,217,3)) #(223,217,3)(300,140,3)
    img = image.img_to_array(img)
    img = img/255.0    
    X.append(img)
    print(npath)    
    #if(count > 50):
      #break
  catcount = catcount + 1

X = np.array(X)
raw_data = {'score': imagecats}
df = pd.DataFrame(raw_data, columns = ['score'])

print(type(X),
type(raw_data),
type(df))

"""# Saving the Data for futher use"""

# np.save("/content/drive/My Drive/project/dataX.npy",X)
# df.to_pickle("/content/drive/My Drive/project/dummy.pkl")
# np.save("/content/drive/My Drive/project/dict.npy", raw_data)

# X = np.load("/content/drive/My Drive/project/dataX.npy")
# df = pd.read_pickle("/content/drive/My Drive/project/dummy.pkl")
# raw_data = np.load("/content/drive/My Drive/project/dict.npy",allow_pickle='TRUE').item()

# Check if the data has loaded 

print(type(X),
type(raw_data),
type(df))

y = df.to_numpy()
print('Shape of y is ',y.shape)
print(y[2])

print('Shape of X is ',X.shape)
plt.imshow(X[2])

"""# Splitting the data into Train Test Validation"""

X_temp, X_val, y_temp, y_val = train_test_split(X, y, random_state = 0, test_size = 0.20,stratify = y)
del X 
X_train, X_test, y_train, y_test = train_test_split(X_temp, y_temp, random_state = 0, test_size = 0.25,stratify = y_temp)

print('Validation Data')
print(X_val.shape,y_val.shape)
unique_elements, counts_elements = np.unique(y_val, return_counts=True)
print(np.asarray((unique_elements, counts_elements)))

print('\n Temporary Data')
print(X_temp.shape,y_temp.shape)
unique_elements, counts_elements = np.unique(y_temp, return_counts=True)
print(np.asarray((unique_elements, counts_elements)))

print('\n Training Data')
print(X_train.shape,y_train.shape)
unique_elements, counts_elements = np.unique(y_train, return_counts=True)
print(np.asarray((unique_elements, counts_elements)))

print('\n Testing Data')
print(X_test.shape,y_test.shape)
unique_elements, counts_elements = np.unique(y_test, return_counts=True)
print(np.asarray((unique_elements, counts_elements)))

"""# **Creating a CNN**"""

#input_shape=input_img

model = Sequential()
model.add(Conv2D(64, kernel_size=(5, 5), strides=(1, 1),
                 activation='relu',
                 input_shape=(223,217,3)))
model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))
model.add(Dropout(0.5))
model.add(Conv2D(128, (5, 5), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.5))
model.add(Flatten())
model.add(Dense(128, activation='relu'))
model.add(Dense(7, activation='softmax'))

model.compile(loss=keras.losses.sparse_categorical_crossentropy,
              optimizer=keras.optimizers.SGD(lr=0.01),
              metrics=['accuracy'])

# model.compile(loss=keras.losses.sparse_categorical_crossentropy,
#               optimizer='adam',
#               metrics=['accuracy'])

model.summary()

class AccuracyHistory(keras.callbacks.Callback):
    def on_train_begin(self, logs={}):
        self.acc = []

    def on_epoch_end(self, batch, logs={}):
        self.acc.append(logs.get('acc'))

history = AccuracyHistory()

history11 = model.fit(X_train, y_train,
          batch_size=128,
          epochs=20,
          verbose=1,
          validation_data=(X_val, y_val),
          callbacks=[history])

print(history11.history.keys())

# model.save("/content/drive/My Drive/project/adam_model.h5")
# model = keras.models.load_model("/content/drive/My Drive/project/adam_model.h5")

"""# **Comparing **"""

#input_shape=input_img

model1 = Sequential()
model1.add(Conv2D(64, kernel_size=(5, 5), strides=(1, 1),
                 activation='relu',
                 input_shape=(223,217,3)))
model1.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))
model1.add(Dropout(0.5))
model1.add(Conv2D(128, (5, 5), activation='relu'))
model1.add(MaxPooling2D(pool_size=(2, 2)))
model1.add(Dropout(0.5))
model1.add(Flatten())
model1.add(Dense(128, activation='relu'))
model1.add(Dense(7, activation='softmax'))

model1.compile(loss=keras.losses.sparse_categorical_crossentropy,
              optimizer=keras.optimizers.SGD(lr=0.1),
              metrics=['accuracy'])

# model.compile(loss=keras.losses.sparse_categorical_crossentropy,
#               optimizer='adam',
#               metrics=['accuracy'])

history12 = model1.fit(X_train, y_train,
          batch_size=128,
          epochs=20,
          verbose=1,
          validation_data=(X_val, y_val),
          callbacks=[history])

#input_shape=input_img

model2 = Sequential()
model2.add(Conv2D(64, kernel_size=(5, 5), strides=(1, 1),
                 activation='relu',
                 input_shape=(223,217,3)))
model2.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))
model2.add(Dropout(0.5))
model2.add(Conv2D(128, (5, 5), activation='relu'))
model2.add(MaxPooling2D(pool_size=(2, 2)))
model2.add(Dropout(0.5))
model2.add(Flatten())
model2.add(Dense(128, activation='relu'))
model2.add(Dense(7, activation='softmax'))

model2.compile(loss=keras.losses.sparse_categorical_crossentropy,
              optimizer=keras.optimizers.SGD(lr=0.3),
              metrics=['accuracy'])

# model.compile(loss=keras.losses.sparse_categorical_crossentropy,
#               optimizer='adam',
#               metrics=['accuracy'])

history13 = model2.fit(X_train, y_train,
          batch_size=128,
          epochs=20,
          verbose=1,
          validation_data=(X_val, y_val),
          callbacks=[history])

#input_shape=input_img

model2 = Sequential()
model2.add(Conv2D(64, kernel_size=(5, 5), strides=(1, 1),
                 activation='relu',
                 input_shape=(223,217,3)))
model2.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))
model2.add(Dropout(0.5))
model2.add(Conv2D(128, (5, 5), activation='relu'))
model2.add(MaxPooling2D(pool_size=(2, 2)))
model2.add(Dropout(0.5))
model2.add(Flatten())
model2.add(Dense(128, activation='relu'))
model2.add(Dense(7, activation='softmax'))

model2.compile(loss=keras.losses.sparse_categorical_crossentropy,
              optimizer=keras.optimizers.SGD(lr=0.03),
              metrics=['accuracy'])

# model.compile(loss=keras.losses.sparse_categorical_crossentropy,
#               optimizer='adam',
#               metrics=['accuracy'])

history14 = model2.fit(X_train, y_train,
          batch_size=128,
          epochs=20,
          verbose=1,
          validation_data=(X_val, y_val),
          callbacks=[history])

#input_shape=input_img

model2 = Sequential()
model2.add(Conv2D(64, kernel_size=(5, 5), strides=(1, 1),
                 activation='relu',
                 input_shape=(223,217,3)))
model2.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))
model2.add(Dropout(0.5))
model2.add(Conv2D(128, (5, 5), activation='relu'))
model2.add(MaxPooling2D(pool_size=(2, 2)))
model2.add(Dropout(0.5))
model2.add(Flatten())
model2.add(Dense(128, activation='relu'))
model2.add(Dense(7, activation='softmax'))

model2.compile(loss=keras.losses.sparse_categorical_crossentropy,
              optimizer=keras.optimizers.SGD(lr=0.003),
              metrics=['accuracy'])

# model.compile(loss=keras.losses.sparse_categorical_crossentropy,
#               optimizer='adam',
#               metrics=['accuracy'])

history15 = model2.fit(X_train, y_train,
          batch_size=128,
          epochs=20,
          verbose=1,
          validation_data=(X_val, y_val),
          callbacks=[history])

#input_shape=input_img

model2 = Sequential()
model2.add(Conv2D(64, kernel_size=(5, 5), strides=(1, 1),
                 activation='relu',
                 input_shape=(223,217,3)))
model2.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))
model2.add(Dropout(0.5))
model2.add(Conv2D(128, (5, 5), activation='relu'))
model2.add(MaxPooling2D(pool_size=(2, 2)))
model2.add(Dropout(0.5))
model2.add(Flatten())
model2.add(Dense(128, activation='relu'))
model2.add(Dense(7, activation='softmax'))

model2.compile(loss=keras.losses.sparse_categorical_crossentropy,
              optimizer=keras.optimizers.SGD(lr=0.0001),
              metrics=['accuracy'])

# model.compile(loss=keras.losses.sparse_categorical_crossentropy,
#               optimizer='adam',
#               metrics=['accuracy'])

history16 = model2.fit(X_train, y_train,
          batch_size=128,
          epochs=20,
          verbose=1,
          validation_data=(X_val, y_val),
          callbacks=[history])

"""# Comparing history11 to history16 model with different learning rates"""

type(history11.history)

plt.plot(history13.history['accuracy']) #lr=0.3
plt.plot(history12.history['accuracy']) #lr=0.1
plt.plot(history14.history['accuracy']) #lr=0.03
plt.plot(history11.history['accuracy']) #lr = 0.01
plt.plot(history15.history['accuracy']) #lr=0.003
plt.plot(history16.history['accuracy']) #lr=0.0001
plt.title('Learning Rates')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['0.3', '0.1','0.03','0.01','0.003', '0.0001'], loc='lower right')
plt.show()

plt.plot(history13.history['loss']) #lr=0.3
plt.plot(history12.history['loss']) #lr=0.1
plt.plot(history14.history['loss']) #lr=0.03
plt.plot(history11.history['loss']) #lr = 0.01
plt.plot(history15.history['loss']) #lr=0.003
plt.plot(history16.history['loss']) #lr=0.0001
plt.title('Model Loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['0.3', '0.1','0.03','0.01','0.003', '0.0001'], loc='lower left')
plt.show()

plt.plot(history13.history['val_loss']) #lr=0.3
plt.plot(history12.history['val_loss']) #lr=0.1
plt.plot(history14.history['val_loss']) #lr=0.03
plt.plot(history11.history['val_loss']) #lr = 0.01
plt.plot(history15.history['val_loss']) #lr=0.003
plt.plot(history16.history['val_loss']) #lr=0.0001
plt.title('Model Validation Loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['0.3', '0.1','0.03','0.01','0.003', '0.0001'], loc='best')
plt.show()

plt.plot(history13.history['val_accuracy']) #lr=0.3
plt.plot(history12.history['val_accuracy']) #lr=0.1
plt.plot(history14.history['val_accuracy']) #lr=0.03
plt.plot(history11.history['val_accuracy']) #lr = 0.01
plt.plot(history15.history['val_accuracy']) #lr=0.003
plt.plot(history16.history['val_accuracy']) #lr=0.0001
plt.title('Learning Rates - Validation')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['0.3', '0.1','0.03','0.01','0.003', '0.0001'], loc='lower right')
plt.show()

"""# **Outputs**

Summarize History for Accuracy

# checking ADAM for Best model
"""

plt.plot(history11.history['accuracy'])
plt.plot(history11.history['val_accuracy'])
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.show()

## ADAM

"""# Accuracy test v/s train for Best model"""

plt.plot(history11.history['accuracy'])
plt.plot(history11.history['val_accuracy'])
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.show()

plt.plot(history14.history['accuracy'])
plt.plot(history14.history['val_accuracy'])
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.show()

"""Summarize History for Loss

# Loss test v/s train for Best model
"""

plt.plot(history11.history['loss'])
plt.plot(history11.history['val_loss'])
plt.title('Model Loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper right')
plt.show()

# test_pred = model.predict_classes(X_test)
# train_pred = model.predict_classes(X_train)
# val_pred = model.predict_classes(X_val)

test_pred = np.argmax(model.predict(X_test), axis=-1)
train_pred = np.argmax(model.predict(X_train), axis=-1)
val_pred = np.argmax(model.predict(X_val), axis=-1)

"""# Confusion Matrix Test-Val-Train"""

print(confusion_matrix(y_train,train_pred))
print(model.evaluate(X_train, y_train, batch_size=128))

print(confusion_matrix(y_test,test_pred))
print(model.evaluate(X_test, y_test, batch_size=128))

print(confusion_matrix(y_val,val_pred))
print(model.evaluate(X_val, y_val, batch_size=128))

"""# Classification Report"""

CATEGORIES = ["Bengali","Hindi","Kannada","Malayalam","Marathi","Tamil","Telugu"]
report = metrics.classification_report(y_test, test_pred, target_names=CATEGORIES)
print(report)

"""# Testing our model on random data(audio files)

Model Testing/Illustration
"""

def create_spectrogram_test(filename,name):
    plt.interactive(False)
    clip, sample_rate = librosa.load(filename, sr=None)
    fig = plt.figure(figsize=[0.72,0.72])
    ax = fig.add_subplot(111)
    ax.axes.get_xaxis().set_visible(False)
    ax.axes.get_yaxis().set_visible(False)
    ax.set_frame_on(False)
    S = librosa.feature.melspectrogram(y=clip, sr=sample_rate)
    librosa.display.specshow(librosa.power_to_db(S, ref=np.max))
    filename  = Path('/content/drive/My Drive/project/test/spec/' + name + '.jpg')
    fig.savefig(filename, dpi=400, bbox_inches='tight',pad_inches=0)
    plt.close()    
    fig.clf()
    plt.close(fig)
    plt.close('all')
    del filename,name,clip,sample_rate,fig,ax,S

import os
import librosa
import librosa.display
from pathlib import Path

def get_audio_paths(path):
    
    audio_paths = []
    
    ls = os.listdir(path) #extracts list of folders in directory 
    for l in ls:
        #iss = os.listdir(os.path.join(path,l)) #extracts filenames 
        #for i in iss:
        audio_paths.append(os.path.join(path,l))

    return audio_paths

path = "/content/drive/My Drive/project/test/audio"
audio_paths = get_audio_paths(path)

test_paths = audio_paths
for i in range(0,len(test_paths)):
  file = test_paths[i]
  filename,name = file,file.split('/')[-1].split('.')[0]
  lang = name.split('_')[0]
  create_spectrogram_test(filename,name)

X_real = []

path = "/content/drive/My Drive/project/test/spec"
rpaths = get_audio_paths(path)


for rpath in rpaths:
  imgr = image.load_img(rpath,grayscale='true',color_mode='rgb',target_size=(223,217,3)) #(223,217,3)(300,140,3)
  imgr = image.img_to_array(imgr)
  imgr = imgr/255.0    
  X_real.append(imgr)

X_real = np.array(X_real)

rpaths

model.predict_classes(X_real)
# CATEGORIES = ["bengali","hindi","kannada","malayalam","marathi","tamil","telugu"]

model.predict(X_real)

"""The model returns probabilities of the respective categories"""

# "bengali"   : 0.01109128,
# "hindi"     : 0.00003309,
# "kannada"   : 0.00803564,
# "malayalam" : 0.01803270,
# "marathi"   : 0.95152813,
# "tamil"     : 0.00093354,
# "telugu"    : 0.00947776

"""# Conclusion 
1. Objective was to build a CNN model that classifies spoken indian languages 
2. Data set had 7000 (1000*7) audio files for 7 different languages which were noise free
3. Audio files were converted to spectrogram (image for audio signals)
4. Image identification model of CNN was developed for these spectrograms 
5. The model with learning rate 0.03 has highest accuracy 98.78 % for test data 
6. We tested the model for random data files.

# Future Scope 
Model can be trained by adding some noise to data and be tested for live data series.
"""